1. What was wrong with the original wordcount-5 data set that made repartitioning worth it? Why did the program run faster after?

The wordcount-5 data set is split into multiple files where some files are significantly larger than others. Repartitioning helps by re-distributing the data more evenly across multiple partitions, ensuring that the executors are handling a more balanced workload.


2. The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower?) Why? [For once, the answer is not “the data set is too small”.]

The same change does not make wordcount-3 run faster because the input is divided into many more files. Repartitioning makes the program run slower because it would lead to a significant number of partitions causing an overhead.


3. How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible? (It's possible to get about another minute off the running time.)

I would break up the input into more files so that there is no single file that is significantly larger than the others.


4. When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the desktop/laptop where you were testing)?

16 partitions: 2:06.18
24 partitions: 2:02.60 
32 partitions: 2:13.32

I found the range between 16 to 32 partitions to perform the best on my laptop with an input size of 1 billion.


5. How much overhead does it seem like Spark adds to a job? How much speedup did PyPy get over the usual Python implementation?

The following results were produced from an input size of 10:

Spark with PyPy: 5.384s
Non-Spark single-threaded with PyPy: 0.038s

It seems like Spark is adding about a 5 second overhead to the job.

The following results were produced from an input size of 1 billion:

Standard CPython implementation: 1:59.40
Spark Python with PyPy: 1:02.61

For PyPy, for significantly large sample sizes (ie. 1 billion), I noticed that it ran roughly twice as fast as the usual Python implementation.