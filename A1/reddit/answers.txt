1. How did the output change when you submitted with -D mapreduce.job.reduces=3? Why would this be necessary if your job produced large output sets?

After submitting the mapreduce job with -D mapreduce.job.reduces=3, I noticed that the output was split into three separate files where each file contains about 1/3 of the final result. I think this would be necessary if the job produced large output sets because it would speed up the job by evenly distributing the data amongst the 3 reducers. As a result, this would allow our 3 reducers to run in parallel and finish the job sooner than if we had only 1 reducer running sequentially.


2. How was the -D mapreduce.job.reduces=0 output different?

After submitting the mapreduce job with -D mapreduce.job.reduces=0, I noticed that the output was showing the intermediate results from the mapping process which showed multiple lines with the same word along with a count of 1 in non-sorted order. In other words, we would be running the job without a reducer and this will not aggregate our counts. We also see the outputs from the multiple mappers in separate output files.


3. Was there any noticeable difference in the running time of your RedditAverage with and without the combiner optimization?

Although there was no noticeable difference in the running times, I did notice that the total time spent by all reduce tasks WITHOUT the combiner was (ms)=3316 and the total time spent by all reduce tasks WITH the combiner was (ms)=3023. This indicates that the reduce tasks run faster when we enable our combiner, and this aligns with our expectations because the combiner's role is to reduce the amount of data sent over the network to the reducer during the shuffle phase.